# bpe v2
this is an upgraded version of the bpe tokenizer in the other folder based on the GPT4 version in [karpathy's minbpe](https://github.com/karpathy/minbpe) (whereas the first version is party based on an older video lesson of his and partly created poorly from scratch by me). In theory I should swap out the original bpe tokenizer with this one, but retraining all those models is not fun so I think i'll wait until I have many more orthogonal upgrades to make before I do so. If you're training a new model and just want the best performance then I recommend you tell it to use this tokenizer; HOWEVER whatever edits you make will then not be comparable to the original templateGPT models I've trained, so you'll also have to train one of those on your own in order to have a fair comparison for your experiment
- possible bug: when I use .display() I noticed that some words that should definitely be full tokens like ' the' get split into for example ' th' and 'e'. is there a bug in the display method? or in my encode or decode functions? ugh idk