{"dim": 192, "device": "cpu", "tokenizer": "bpe_v1", "vocab_len": 8192, "num_layers": 4, "second_resid_norm": false, "mlp_hidden_mult": 3.5, "mlp_bias": false, "mlp_nonlinearity": "SiLU", "mlp_gated": false, "num_q_heads": 2, "num_kv_heads": 1, "head_dim": 96, "theta": 10000, "max_seq_len": 512, "scale_first_resid": true, "norm_type": "LayerNorm", "norm_affine": true, "norm_bias": true, "eps": 1e-06, "max_batch_size": 1}