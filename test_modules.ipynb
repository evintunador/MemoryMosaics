{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea126987-59aa-4f76-b926-6d632887c30b",
   "metadata": {},
   "source": [
    "# This notebook is designed for teaching/testing purposes to help you visualize the tensor shapes that go through each module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f833b1f8-ea91-4ae5-b3a3-73e08e4c8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that. \n",
    "# you prolly won't need this cell but running it won't hurt anything either\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c2c04f-2dbd-4020-8d91-cc0e4e8511b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelConfig(dim=128, device='mps', tokenizer='bpe_v1', vocab_len=8192, num_layers=4, second_resid_norm=False, num_heads=4, head_dim=32, max_seq_len=128, mm_bias=False, pmem_size=336, pmem_count=2, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06)\n"
     ]
    }
   ],
   "source": [
    "# config file\n",
    "from config import ModelConfig\n",
    "cfg = ModelConfig()\n",
    "print(cfg)\n",
    "\n",
    "# import the tokenizer specified by cfg\n",
    "from tools import import_from_nested_path\n",
    "imported_objects = import_from_nested_path(['tokenizers', cfg.tokenizer], 'tokenizer', ['get_tokenizer'])\n",
    "get_tokenizer = imported_objects.get('get_tokenizer')\n",
    "tokenizer = get_tokenizer(size = 512) # assuming 'bpe', size options are 95, 128, 256, 512, 1024 and 2048\n",
    "\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c12e3b-dc63-4479-ad55-b05d96364d1f",
   "metadata": {},
   "source": [
    "# Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3debc7b1-a7ec-4fb3-98cb-d16edf7c71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.norm import Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627969e9-9017-43f3-90ec-9a485abef26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.256 K parameters\n",
      "Norm()\n",
      "\n",
      "==========Entering Norm.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "\n",
      "==========Entering Norm.RMSNorm==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 128])\n",
      "==========Exiting Norm.RMSNorm==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 128])\n",
      "==========Exiting Norm.forward==========\n",
      "CPU times: user 61.1 ms, sys: 41.3 ms, total: 102 ms\n",
      "Wall time: 110 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### RMSNorm\n",
    "\n",
    "# Create an instance of RMSNorm\n",
    "module = Norm(cfg.dim, 'RMSNorm').to(cfg.device)\n",
    "\n",
    "# let's take a look\n",
    "print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')\n",
    "print(module)\n",
    "\n",
    "# Initially, logging is disabled by default\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('CosineNorm')\n",
    "#module.disable_function_logging('LayerNorm')\n",
    "#module.disable_function_logging('RMSNorm')\n",
    "\n",
    "x = torch.randn(32,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "output = module(x)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fe527a-e933-4601-b911-5911f2769f9b",
   "metadata": {},
   "source": [
    "# Leaky Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e770167-dfce-49c9-a69f-93da79b6f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.memory_mosaic import LeakyAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "125e10ad-3154-45c5-9f01-1628b57d26ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004 K parameters\n",
      "LeakyAvg()\n",
      "\n",
      "==========Entering LeakyAvg.forward==========\n",
      "Inputs:\n",
      "Tensor 'k' shape: torch.Size([32, 4, 128, 32])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting LeakyAvg.forward==========\n",
      "CPU times: user 35.9 ms, sys: 5.5 ms, total: 41.4 ms\n",
      "Wall time: 44.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create an instance of context memory\n",
    "module = LeakyAvg(cfg.max_seq_len, cfg.num_heads).to(cfg.device)\n",
    "print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')\n",
    "print(module)\n",
    "\n",
    "# Initially, logging is disabled by default\n",
    "module.enable_logging()\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('')\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "x = torch.randn(32,cfg.num_heads,cfg.max_seq_len,cfg.dim // cfg.num_heads).to(cfg.device)\n",
    "output = module(x)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1196f840-1acf-456a-a153-96b4f8cfe630",
   "metadata": {},
   "source": [
    "# Key Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "325fd3a6-ef52-4777-b2b8-11f167b74905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.memory_mosaic import KeyFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24971d13-dc0a-4cf4-b7e3-2014794d5ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.392 K parameters\n",
      "KeyFeatureExtractor(\n",
      "  (W_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (leaky_avg): LeakyAvg()\n",
      ")\n",
      "\n",
      "==========Entering KeyFeatureExtractor.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "\n",
      "==========Entering KeyFeatureExtractor.make_key==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting KeyFeatureExtractor.make_key==========\n",
      "\n",
      "==========Entering LeakyAvg.forward==========\n",
      "Inputs:\n",
      "Tensor 'k' shape: torch.Size([32, 4, 128, 32])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting LeakyAvg.forward==========\n",
      "\n",
      "==========Entering KeyFeatureExtractor.scale_key==========\n",
      "Inputs:\n",
      "Tensor 'k' shape: torch.Size([32, 4, 128, 32])\n",
      "Integer 'scale_pow': Value=1\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting KeyFeatureExtractor.scale_key==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting KeyFeatureExtractor.forward==========\n",
      "CPU times: user 41.1 ms, sys: 5.1 ms, total: 46.2 ms\n",
      "Wall time: 51.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create an instance of context memory\n",
    "module = KeyFeatureExtractor(\n",
    "    cfg.num_heads, \n",
    "    cfg.head_dim,\n",
    "    cfg.dim, \n",
    "    cfg.mm_bias, \n",
    "    cfg.max_seq_len\n",
    ").to(cfg.device)\n",
    "print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')\n",
    "print(module)\n",
    "\n",
    "# Initially, logging is disabled by default\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('make_key')\n",
    "#module.disable_function_logging('scale_key')\n",
    "### enabling printing for sub-modules\n",
    "module.leaky_avg.enable_logging()\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "x = torch.randn(32,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "output = module(x)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5c6582-1d4f-4bcd-a1ee-1d64c6a6852b",
   "metadata": {},
   "source": [
    "# Value Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2472110-e185-484e-82fd-c8fa9cbe19db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.memory_mosaic import ValFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6273eb2-82b3-44a4-9914-2370ca2b9538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.392 K parameters\n",
      "ValFeatureExtractor(\n",
      "  (W_v): Linear(in_features=128, out_features=128, bias=False)\n",
      ")\n",
      "\n",
      "==========Entering ValFeatureExtractor.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "\n",
      "==========Entering ValFeatureExtractor.make_val==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 32, 128])\n",
      "==========Exiting ValFeatureExtractor.make_val==========\n",
      "\n",
      "==========Entering ValFeatureExtractor.scale_val==========\n",
      "Inputs:\n",
      "Tensor 'v' shape: torch.Size([32, 4, 32, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting ValFeatureExtractor.scale_val==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting ValFeatureExtractor.forward==========\n",
      "CPU times: user 27.8 ms, sys: 3.54 ms, total: 31.4 ms\n",
      "Wall time: 33.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create an instance of context memory\n",
    "module = ValFeatureExtractor(\n",
    "    cfg.num_heads, \n",
    "    cfg.head_dim,\n",
    "    cfg.dim, \n",
    "    cfg.mm_bias\n",
    ").to(cfg.device)\n",
    "print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')\n",
    "print(module)\n",
    "\n",
    "# Initially, logging is disabled by default\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('make_val')\n",
    "#module.disable_function_logging('scale_val')\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "x = torch.randn(32,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "output = module(x)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e190f3de-37fd-442b-bfb1-6a090115fc75",
   "metadata": {},
   "source": [
    "# Context Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a89e1e8f-cedd-4885-ad50-934827ed045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.memory_mosaic import ContextMem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e8f27b4-6d1f-4fcd-99b0-2284e65d6b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.168 K parameters\n",
      "ContextMem(\n",
      "  (k_featurizer): KeyFeatureExtractor(\n",
      "    (W_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (leaky_avg): LeakyAvg()\n",
      "  )\n",
      "  (v_featurizer): ValFeatureExtractor(\n",
      "    (W_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "  )\n",
      "  (c_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "==========Entering ContextMem.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "\n",
      "==========Entering KeyFeatureExtractor.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "\n",
      "==========Entering KeyFeatureExtractor.make_key==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting KeyFeatureExtractor.make_key==========\n",
      "\n",
      "==========Entering LeakyAvg.forward==========\n",
      "Inputs:\n",
      "Tensor 'k' shape: torch.Size([32, 4, 128, 32])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting LeakyAvg.forward==========\n",
      "\n",
      "==========Entering KeyFeatureExtractor.scale_key==========\n",
      "Inputs:\n",
      "Tensor 'k' shape: torch.Size([32, 4, 128, 32])\n",
      "Integer 'scale_pow': Value=1\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting KeyFeatureExtractor.scale_key==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting KeyFeatureExtractor.forward==========\n",
      "\n",
      "==========Entering ValFeatureExtractor.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "\n",
      "==========Entering ValFeatureExtractor.make_val==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 32, 128])\n",
      "==========Exiting ValFeatureExtractor.make_val==========\n",
      "\n",
      "==========Entering ValFeatureExtractor.scale_val==========\n",
      "Inputs:\n",
      "Tensor 'v' shape: torch.Size([32, 4, 32, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting ValFeatureExtractor.scale_val==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting ValFeatureExtractor.forward==========\n",
      "\n",
      "==========Entering ContextMem.attend==========\n",
      "Inputs:\n",
      "Tensor 'k' shape: torch.Size([32, 4, 128, 32])\n",
      "Integer 'seq_len': Value=128\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 127, 128])\n",
      "==========Exiting ContextMem.attend==========\n",
      "\n",
      "==========Entering ContextMem.proj_vals==========\n",
      "Inputs:\n",
      "Tensor 'att' shape: torch.Size([32, 4, 127, 128])\n",
      "Tensor 'v' shape: torch.Size([32, 4, 128, 32])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting ContextMem.proj_vals==========\n",
      "\n",
      "==========Entering ContextMem.reassemble_heads==========\n",
      "Inputs:\n",
      "Tensor 'y' shape: torch.Size([32, 4, 128, 32])\n",
      "Integer 'batch_size': Value=32\n",
      "Integer 'seq_len': Value=128\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 128])\n",
      "==========Exiting ContextMem.reassemble_heads==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 128])\n",
      "==========Exiting ContextMem.forward==========\n",
      "CPU times: user 45.2 ms, sys: 7.83 ms, total: 53.1 ms\n",
      "Wall time: 58.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create an instance of context memory\n",
    "module = ContextMem(\n",
    "    cfg.num_heads, \n",
    "    cfg.head_dim,\n",
    "    cfg.dim, \n",
    "    cfg.mm_bias, \n",
    "    cfg.max_seq_len, \n",
    "    cfg.dropout_rate\n",
    ").to(cfg.device)\n",
    "print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')\n",
    "print(module)\n",
    "\n",
    "# Initially, logging is disabled by default\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('proj_vals')\n",
    "#module.disable_function_logging('reassemble_heads')\n",
    "### enabling printing for sub-modules\n",
    "module.k_featurizer.enable_logging()\n",
    "module.k_featurizer.leaky_avg.enable_logging()\n",
    "module.v_featurizer.enable_logging()\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "x = torch.randn(32,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "output = module(x)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb308c5-b578-46f2-86ae-bfa6800be641",
   "metadata": {},
   "source": [
    "# Persistent Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7736b685-f941-4182-a5b7-4731cce706b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.memory_mosaic import PersistentMem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c62e49e9-2189-4269-968e-1df99469dc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204.812 K parameters\n",
      "PersistentMem(\n",
      "  (k_featurizer): KeyFeatureExtractor(\n",
      "    (W_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (leaky_avg): LeakyAvg()\n",
      "  )\n",
      "  (c_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "==========Entering PersistentMem.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "\n",
      "==========Entering KeyFeatureExtractor.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "Integer 'scale_pow': Value=2\n",
      "\n",
      "==========Entering KeyFeatureExtractor.make_key==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting KeyFeatureExtractor.make_key==========\n",
      "\n",
      "==========Entering LeakyAvg.forward==========\n",
      "Inputs:\n",
      "Tensor 'k' shape: torch.Size([32, 4, 128, 32])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting LeakyAvg.forward==========\n",
      "\n",
      "==========Entering KeyFeatureExtractor.scale_key==========\n",
      "Inputs:\n",
      "Tensor 'k' shape: torch.Size([32, 4, 128, 32])\n",
      "Integer 'scale_pow': Value=2\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting KeyFeatureExtractor.scale_key==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting KeyFeatureExtractor.forward==========\n",
      "\n",
      "==========Entering PersistentMem.attend==========\n",
      "Inputs:\n",
      "Integer 'y': Value=0\n",
      "Tensor 'k' shape: torch.Size([32, 4, 128, 32])\n",
      "Integer 'i': Value=0\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 336])\n",
      "==========Exiting PersistentMem.attend==========\n",
      "\n",
      "==========Entering PersistentMem.proj_val==========\n",
      "Inputs:\n",
      "Tensor 'att' shape: torch.Size([32, 4, 128, 336])\n",
      "Integer 'i': Value=0\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting PersistentMem.proj_val==========\n",
      "\n",
      "==========Entering PersistentMem.attend==========\n",
      "Inputs:\n",
      "Tensor 'y' shape: torch.Size([32, 4, 128, 32])\n",
      "Tensor 'k' shape: torch.Size([32, 4, 128, 32])\n",
      "Integer 'i': Value=1\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 336])\n",
      "==========Exiting PersistentMem.attend==========\n",
      "\n",
      "==========Entering PersistentMem.proj_val==========\n",
      "Inputs:\n",
      "Tensor 'att' shape: torch.Size([32, 4, 128, 336])\n",
      "Integer 'i': Value=1\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting PersistentMem.proj_val==========\n",
      "\n",
      "==========Entering PersistentMem.scale==========\n",
      "Inputs:\n",
      "Tensor 'y' shape: torch.Size([32, 4, 128, 32])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 128, 32])\n",
      "==========Exiting PersistentMem.scale==========\n",
      "\n",
      "==========Entering PersistentMem.reassemble_heads==========\n",
      "Inputs:\n",
      "Tensor 'y' shape: torch.Size([32, 4, 128, 32])\n",
      "Integer 'batch_size': Value=32\n",
      "Integer 'seq_len': Value=128\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 128])\n",
      "==========Exiting PersistentMem.reassemble_heads==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 128])\n",
      "==========Exiting PersistentMem.forward==========\n",
      "CPU times: user 56.6 ms, sys: 17.7 ms, total: 74.3 ms\n",
      "Wall time: 74.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create an instance of context memory\n",
    "module = PersistentMem(\n",
    "    cfg.num_heads, \n",
    "    cfg.head_dim,\n",
    "    cfg.dim, \n",
    "    cfg.mm_bias, \n",
    "    cfg.max_seq_len, \n",
    "    cfg.pmem_count, \n",
    "    cfg.pmem_size, \n",
    "    cfg.dropout_rate\n",
    ").to(cfg.device)\n",
    "print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')\n",
    "print(module)\n",
    "\n",
    "# Initially, logging is disabled by default\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('proj_val')\n",
    "#module.disable_function_logging('scale')\n",
    "#module.disable_function_logging('reassemble_heads')\n",
    "### enabling printing for sub-modules\n",
    "module.k_featurizer.enable_logging()\n",
    "module.k_featurizer.leaky_avg.enable_logging()\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "x = torch.randn(32,cfg.max_seq_len,cfg.dim).to(cfg.device).to(cfg.device)\n",
    "output = module(x)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3a502f-4646-4a02-9412-372482af9fa0",
   "metadata": {},
   "source": [
    "# ResidualLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a21d708-af47-4f32-b111-08efedc584f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.layer import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c886661d-5a26-4787-93d1-c3c8c6b1c02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254.492 K parameters\n",
      "Layer(\n",
      "  (pre_context_norm): Norm()\n",
      "  (context): ContextMem(\n",
      "    (k_featurizer): KeyFeatureExtractor(\n",
      "      (W_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "      (leaky_avg): LeakyAvg()\n",
      "    )\n",
      "    (v_featurizer): ValFeatureExtractor(\n",
      "      (W_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "    )\n",
      "    (c_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (pre_persistent_norm): Norm()\n",
      "  (persistent): PersistentMem(\n",
      "    (k_featurizer): KeyFeatureExtractor(\n",
      "      (W_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "      (leaky_avg): LeakyAvg()\n",
      "    )\n",
      "    (c_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "==========Entering Layer.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "Integer 'training': Value=True\n",
      "\n",
      "==========Entering Layer.context_connect==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 128])\n",
      "==========Exiting Layer.context_connect==========\n",
      "\n",
      "==========Entering Layer.persistent_connect==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 128])\n",
      "==========Exiting Layer.persistent_connect==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 128])\n",
      "==========Exiting Layer.forward==========\n",
      "CPU times: user 52 ms, sys: 50.9 ms, total: 103 ms\n",
      "Wall time: 169 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TRAINING\n",
    "module = Layer(cfg).to(cfg.device)\n",
    "print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('context_connect')\n",
    "#module.disable_function_logging('persistent_connect')\n",
    "### enabling printing for sub-modules\n",
    "#module.pre_context_norm.enable_logging()\n",
    "#module.context.enable_logging()\n",
    "#module.post_context_norm.enable_logging()\n",
    "#module.pre_persistent_norm.enable_logging()\n",
    "#module.persistent.enable_logging()\n",
    "#module.post_persistent_norm.enable_logging()\n",
    "\n",
    "x = torch.randn(32,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "output = module(x, training=True)\n",
    "module.disable_logging()\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c677ac2b-06d0-4895-b718-2bc664613c98",
   "metadata": {},
   "source": [
    "# Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "655a1fec-4e32-4c7a-86e0-1390a03e88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ccc88d4-c650-43d3-85ee-e68152cb2e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2067.184 K parameters\n",
      "Model(\n",
      "  (token_embedder): Embedding(8195, 128)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x Layer(\n",
      "      (pre_context_norm): Norm()\n",
      "      (context): ContextMem(\n",
      "        (k_featurizer): KeyFeatureExtractor(\n",
      "          (W_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (leaky_avg): LeakyAvg()\n",
      "        )\n",
      "        (v_featurizer): ValFeatureExtractor(\n",
      "          (W_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (c_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (pre_persistent_norm): Norm()\n",
      "      (persistent): PersistentMem(\n",
      "        (k_featurizer): KeyFeatureExtractor(\n",
      "          (W_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (leaky_avg): LeakyAvg()\n",
      "        )\n",
      "        (c_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "\n",
      "==========Entering Model.forward==========\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([32, 128])\n",
      "Tensor 'target_token_ids' shape: torch.Size([32, 128])\n",
      "\n",
      "==========Entering Layer.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "Integer 'training': Value=True\n",
      "\n",
      "==========Entering Layer.context_connect==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 128])\n",
      "==========Exiting Layer.context_connect==========\n",
      "\n",
      "==========Entering Layer.persistent_connect==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 128])\n",
      "==========Exiting Layer.persistent_connect==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 128])\n",
      "==========Exiting Layer.forward==========\n",
      "\n",
      "==========Entering Norm.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "\n",
      "==========Entering Norm.RMSNorm==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 128, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 128])\n",
      "==========Exiting Norm.RMSNorm==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 128, 128])\n",
      "==========Exiting Norm.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 128, 8195])\n",
      "Tensor 'output[1]' shape: torch.Size([])\n",
      "==========Exiting Model.forward==========\n",
      "tensor(127.5291, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "CPU times: user 167 ms, sys: 119 ms, total: 286 ms\n",
      "Wall time: 504 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TRAINING\n",
    "module = Model(cfg).to(cfg.device)\n",
    "print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "### enabling printing for sub-modules\n",
    "module.layers[0].enable_logging() # we'll only look at one layer\n",
    "module.final_norm.enable_logging()\n",
    "\n",
    "input_token_ids = torch.randint(tokenizer.vocab_len, (32, cfg.max_seq_len)).to(cfg.device)\n",
    "target_token_ids = torch.randint(tokenizer.vocab_len, (32, cfg.max_seq_len)).to(cfg.device)\n",
    "\n",
    "output, loss = module(input_token_ids, target_token_ids=target_token_ids)\n",
    "print(loss)\n",
    "module.disable_logging()\n",
    "del module, input_token_ids, target_token_ids, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d82f9c-e531-47b5-8ffb-c8374ffb0e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
